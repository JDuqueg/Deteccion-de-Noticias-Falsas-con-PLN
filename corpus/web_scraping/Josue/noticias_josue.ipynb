{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c815c48a",
   "metadata": {},
   "source": [
    "# Noticias verdaderas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88904e0",
   "metadata": {},
   "source": [
    "## Infobae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dbe79e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardada: Verdad\\noticia_1.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_2.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_3.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_4.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_5.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_6.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_7.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_8.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_9.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_10.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_11.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_12.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_13.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_14.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_15.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_16.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_17.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_18.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_19.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_20.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_21.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_22.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_23.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_24.txt\n",
      "‚úÖ Guardada: Verdad\\noticia_25.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def obtener_links(seccion=\"colombia\", limite=25):\n",
    "    \"\"\"\n",
    "    Extrae hasta `limite` links de noticias de Infobae en la secci√≥n indicada,\n",
    "    usando directamente la clase 'story-card-ctn'.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.infobae.com/{seccion}/\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", class_=\"story-card-ctn\", href=True):\n",
    "        href = a[\"href\"]\n",
    "\n",
    "        # Normalizar: agregar dominio si es relativo\n",
    "        if href.startswith(\"/\"):\n",
    "            href = \"https://www.infobae.com\" + href\n",
    "\n",
    "        if href not in links:\n",
    "            links.append(href)\n",
    "\n",
    "        if len(links) >= limite:\n",
    "            break\n",
    "\n",
    "    return links\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extraer_noticia(url):\n",
    "    \"\"\"\n",
    "    Extrae el t√≠tulo y el cuerpo de una noticia de Infobae.\n",
    "    \n",
    "    Par√°metros:\n",
    "        url (str): URL de la noticia\n",
    "    \n",
    "    Retorna:\n",
    "        list: [titulo, cuerpo]\n",
    "    \"\"\"\n",
    "    # Hacer la petici√≥n\n",
    "    response = requests.get(url, timeout=10)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error al acceder a la p√°gina: {response.status_code}\")\n",
    "\n",
    "    # Parsear HTML\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extraer t√≠tulo (h1 con clase 'article-headline')\n",
    "    titulo = soup.find(\"h1\", class_=\"article-headline\")\n",
    "    titulo = titulo.get_text(strip=True) if titulo else \"No encontrado\"\n",
    "\n",
    "    # Extraer p√°rrafos dentro del div con clase \"body-article\"\n",
    "    cuerpo_div = soup.find(\"div\", class_=\"body-article\")\n",
    "    cuerpo = \"\"\n",
    "    if cuerpo_div:\n",
    "        parrafos = cuerpo_div.find_all(\"p\", class_=\"paragraph\")\n",
    "        cuerpo = \" \".join([p.get_text(' ',strip=True) for p in parrafos])\n",
    "\n",
    "    return titulo,cuerpo\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "def guardar_noticias(links, carpeta=\"Verdad\"):\n",
    "    if not os.path.exists(carpeta):\n",
    "        os.makedirs(carpeta)\n",
    "\n",
    "    def limpiar_texto(texto):\n",
    "        \"\"\"\n",
    "        Elimina frases promocionales como:\n",
    "        'Ahora puede seguirnos enFacebooky en nuestroWhatsApp Channel'\n",
    "        \"\"\"\n",
    "        # Regex que elimina ese bloque (con o sin espacios entre palabras)\n",
    "        patron = r\"Ahora puede seguirnos.*?(Facebook).*?(WhatsApp Channel)\"\n",
    "        return re.sub(patron, \"\", texto, flags=re.DOTALL)\n",
    "\n",
    "    for i, link in enumerate(links, start=1):\n",
    "        try:\n",
    "            titulo, cuerpo = extraer_noticia(link)\n",
    "\n",
    "            # Limpiar texto con regex\n",
    "            cuerpo_limpio = limpiar_texto(cuerpo)\n",
    "\n",
    "            # Guardar\n",
    "            nombre_archivo = f\"noticia_{i}.txt\"\n",
    "            ruta_archivo = os.path.join(carpeta, nombre_archivo)\n",
    "\n",
    "            with open(ruta_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(titulo + \"\\n\")\n",
    "                f.write(cuerpo_limpio)\n",
    "\n",
    "            print(f\"‚úÖ Guardada: {ruta_archivo}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error con {link}: {e}\")\n",
    "\n",
    "noticias = obtener_links()\n",
    "guardar_noticias(noticias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d63c6",
   "metadata": {},
   "source": [
    "# Noticias Falsas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a4d97bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardada noticia 1: Chilangos que compraron bolillo para prevenir tendr√°n que co...\n",
      "‚úÖ Guardada noticia 2: Tras √©xito, Demon Slayer confirma 3 secuelas, 2 spin-offs y ...\n",
      "‚úÖ Guardada noticia 3: Presentadores de noticias en EU deber√°n alabar a Trump cada ...\n",
      "‚úÖ Guardada noticia 4: Inauguran la Presa Josefa Ortiz T√©llez-Tir√≥n, Soltera y Empo...\n",
      "‚úÖ Guardada noticia 5: Pozole que sobr√≥ no podr√° ser congelado hasta las Posadas: S...\n",
      "‚úÖ Guardada noticia 6: 90% de los mexicanos solo quieren ver que los pol√≠ticos se e...\n",
      "‚úÖ Guardada noticia 7: ¬øAlgo m√°s, patr√≥n? Elevan nivel de r√≠o para que vicepresiden...\n",
      "‚úÖ Guardada noticia 8: So, it begins: Inteligencia Artificial intent√≥ copiarse por ...\n",
      "‚úÖ Guardada noticia 9: ¬øNo que muy ac√°? Consola de 1977 vence a ChatGPT en partida ...\n",
      "‚ùå Saltada (sin contenido): https://eldeforma.com/2025/04/16/que-son-las-ptu/\n",
      "‚ùå Saltada (sin contenido): https://eldeforma.com/2025/04/14/lego-leon/\n",
      "‚ùå Saltada (sin contenido): https://eldeforma.com/2025/02/21/metodos-efectivos-para-ahorrar-pero-dandote-tus-gustitos/\n",
      "‚ùå Saltada (sin contenido): https://eldeforma.com/2025/02/17/elije-tu-fav-esa-es-la-cuestion/\n",
      "‚ùå Saltada (sin contenido): https://eldeforma.com/2025/01/30/contenido-no-pagado-2/\n",
      "‚ùå Saltada (sin contenido): https://eldeforma.com/2025/01/30/contenido-no-pagado/\n",
      "‚úÖ Guardada noticia 10: Tras anuncio de port de Mario Galaxy, fans de Nintendo son n...\n",
      "‚úÖ Guardada noticia 11: Mexicanos que no hayan amanecido crudos no podr√°n celebrar e...\n",
      "‚úÖ Guardada noticia 12: Tras incidente con socav√≥n, Jarritos ya planea mover su prod...\n",
      "‚úÖ Guardada noticia 13: Canelo √Ålvarez listo para interpretar a ‚ÄúDon Ram√≥n‚Äù en reboo...\n",
      "‚úÖ Guardada noticia 14: FGR detendr√° a personas que hayan ido al concierto de Oasis ...\n",
      "‚úÖ Guardada noticia 15: Llegan los hermanos Gallagher a la Terminal 1 del AICM...\n",
      "‚úÖ Guardada noticia 16: Jair Bolsonaro asegura que a los 90 son ‚Äúlos a√±os dorados‚Äù...\n",
      "‚úÖ Guardada noticia 17: Formas de identificar a un curioso...\n",
      "‚úÖ Guardada noticia 18: Canciones adem√°s de las de Paquita ‚Äúla del Barrio‚Äù que dan s...\n",
      "‚úÖ Guardada noticia 19: Momentos por los que siempre recordaremos a Paquita la del B...\n",
      "‚úÖ Guardada noticia 20: 7 veces adem√°s de ‚ÄúJohanne Sacreblu‚Äù que los franceses fuero...\n",
      "‚úÖ Guardada noticia 21: Unos reguetoneros se subieron a cantar al metro y un abuelit...\n",
      "‚úÖ Guardada noticia 22: Playa Limbo estuvo en el Deforma y hasta sentimos que and√°ba...\n",
      "‚úÖ Guardada noticia 23: Por si quer√≠an gozo en la vida: Kacey Musgraves se presentar...\n",
      "‚úÖ Guardada noticia 24: AIR se apoder√≥ del Auditorio Nacional para darnos un safari ...\n",
      "‚úÖ Guardada noticia 25: Babas√≥nicos: regresaron al Auditorio Nacional y se llevaron ...\n",
      "\n",
      "üìå Total de noticias guardadas: 25\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Funci√≥n para obtener los links de noticias de El Deforma\n",
    "def obtener_links_deforma(url, limite=35):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Buscar todos los <a> que contengan links v√°lidos\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"eldeforma.com\" in href and \"/2025/\" in href:  # solo noticias recientes\n",
    "            if href not in links:  # evitar duplicados\n",
    "                links.append(href)\n",
    "        if len(links) >= limite:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extraer_noticia_deforma(url):\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        resp.encoding = \"utf-8\"\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        articulo = soup.find(\"article\")\n",
    "        if not articulo:\n",
    "            return None, None\n",
    "\n",
    "        # Buscar contenedor con p√°rrafos\n",
    "        contenedor = None\n",
    "        for div in articulo.find_all(\"div\"):\n",
    "            if div.find(\"p\"):\n",
    "                contenedor = div\n",
    "                break\n",
    "        if not contenedor:\n",
    "            return None, None\n",
    "\n",
    "        # Eliminar basura\n",
    "        for tag in contenedor.find_all([\"blockquote\", \"iframe\", \"script\", \"figure\"]):\n",
    "            tag.decompose()\n",
    "        for tag in contenedor.find_all(\"div\", attrs={\"data-testid\": \"tweetText\"}):\n",
    "            tag.decompose()\n",
    "\n",
    "        titulo = soup.find(\"h1\")\n",
    "        titulo = titulo.get_text(strip=True) if titulo else None\n",
    "\n",
    "        parrafos = [p.get_text(\" \", strip=True) for p in contenedor.find_all(\"p\")]\n",
    "        cuerpo = \" \".join(parrafos)\n",
    "\n",
    "        # Limpieza regex\n",
    "        cuerpo = re.sub(r'https?://\\S+', '', cuerpo)\n",
    "        cuerpo = re.sub(r'@\\w+', '', cuerpo)\n",
    "        cuerpo = re.sub(r'#\\w+', '', cuerpo)\n",
    "        cuerpo = re.sub(r'\\s+', ' ', cuerpo).strip()\n",
    "\n",
    "        if not cuerpo:\n",
    "            return None, None\n",
    "\n",
    "        return titulo, cuerpo\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def guardar_noticias_deforma(links, carpeta=\"Falsas\", n=25):\n",
    "    if not os.path.exists(carpeta):\n",
    "        os.makedirs(carpeta)\n",
    "\n",
    "    contador = 0\n",
    "    for url in links:\n",
    "        if contador >= n:\n",
    "            break\n",
    "\n",
    "        titulo, cuerpo = extraer_noticia_deforma(url)\n",
    "\n",
    "        if titulo and cuerpo:\n",
    "            # Nombre del archivo limpio\n",
    "            nombre_archivo = f\"noticia_{contador+1}.txt\"\n",
    "            ruta = os.path.join(carpeta, nombre_archivo)\n",
    "\n",
    "            with open(ruta, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(titulo + \"\\n\\n\" + cuerpo)\n",
    "\n",
    "            contador += 1\n",
    "            print(f\"‚úÖ Guardada noticia {contador}: {titulo[:60]}...\")\n",
    "        else:\n",
    "            print(f\"‚ùå Saltada (sin contenido): {url}\")\n",
    "\n",
    "    print(f\"\\nüìå Total de noticias guardadas: {contador}\")\n",
    "\n",
    "links = obtener_links_deforma(\"https://eldeforma.com/\")\n",
    "guardar_noticias_deforma(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3825cd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON generado en verdad.txt con 25 noticias\n",
      "‚úÖ JSON generado en falsas.txt con 25 noticias\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def generar_json(carpeta, archivo_salida, label):\n",
    "    datos = []\n",
    "\n",
    "    # Procesar carpeta de noticias falsas\n",
    "    for archivo in os.listdir(carpeta):\n",
    "        if archivo.endswith(\".txt\"):\n",
    "            ruta = os.path.join(carpeta, archivo)\n",
    "            with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "                lineas = f.readlines()\n",
    "                titulo = lineas[0].strip() if lineas else \"Sin t√≠tulo\"\n",
    "                cuerpo = \" \".join(linea.strip() for linea in lineas[1:])\n",
    "            \n",
    "            datos.append({\n",
    "                \"label\": label,\n",
    "                \"title\": titulo,\n",
    "                \"body\": cuerpo\n",
    "            })\n",
    "\n",
    "    # Guardar en un √∫nico archivo JSON\n",
    "    with open(archivo_salida, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(datos, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ JSON generado en {archivo_salida} con {len(datos)} noticias\")\n",
    "\n",
    "\n",
    "generar_json(\"Verdad\", \"verdad.txt\", 1)\n",
    "generar_json(\"Falsas\", 'falsas.txt', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527d06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
