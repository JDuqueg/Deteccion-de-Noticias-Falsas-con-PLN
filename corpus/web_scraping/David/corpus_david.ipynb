{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb57225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7942ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_FALSE = \"https://www.bbc.com/mundo/topics/c95y3rnvxkwt\"\n",
    "TOPIC_TRUE  = \"https://www.bbc.com/mundo/topics/c7zp57yyz25t\"\n",
    "\n",
    "OUTDIR = Path(\"corpus_bbc_mundo\")\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "OUT_FALSE_FILE = OUTDIR / \"Falsas_25.txt\"\n",
    "OUT_TRUE_FILE  = OUTDIR / \"Verdaderas_25.txt\"\n",
    "\n",
    "LABEL_FALSE = 0\n",
    "LABEL_TRUE  = 1\n",
    "\n",
    "TARGET_PER_LABEL = 25\n",
    "RATE_SLEEP = 0.8\n",
    "\n",
    "# Robust defaults (ajusta si necesitas más)\n",
    "MAX_PAGES = 40            # cuántas páginas ?page=N probar\n",
    "SCROLL_PAUSES = 10        # cuántos ciclos de scroll por página\n",
    "SCROLL_INTERVAL = 0.6     # segundos entre cada scroll paso\n",
    "WAIT_TIMEOUT = 12         # segundos para WebDriverWait\n",
    "\n",
    "# Recognize both /articles/ and /noticias- style links\n",
    "LINK_PATTERNS = [\"/articles/\", \"/noticias-\"]\n",
    "\n",
    "# Truncado y limpieza de boilerplate\n",
    "TRUNCATE_PHRASES = [\n",
    "    \"haz clic\", \"suscríb\", \"recibe el mejor contenido\", \"también puedes seguirnos\",\n",
    "    \"y recuerda\", \"descarga la última versión\", \"más leídas\", \"fin de\", \"newsletter\"\n",
    "]\n",
    "REMOVE_LINE_PATTERNS = [\n",
    "    r\"^Fuente de la imagen[,;:]?.*$\",\n",
    "    r\"^Imagen de .*$\",\n",
    "    r\"^(También|Y) recuerda.*$\",\n",
    "    r\"^(Haz clic|Haz clic para).*$\",\n",
    "    r\"^(Suscríbete|Suscribete|Suscríbase).*$\",\n",
    "    r\"^Recibe el mejor contenido.*$\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97427374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver(headless=True):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--lang=es-ES\")\n",
    "    # hacer más \"humano\"\n",
    "    opts.add_argument(\"start-maximized\")\n",
    "    opts.add_argument(\"disable-infobars\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option('useAutomationExtension', False)\n",
    "    # evita ciertos bloqueos básicos\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "def short_id(url: str) -> str:\n",
    "    return hashlib.sha1(url.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "def aggressive_scroll(driver):\n",
    "    \"\"\"Scroll incremental para forzar carga lazy.\"\"\"\n",
    "    try:\n",
    "        for i in range(SCROLL_PAUSES):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight * arguments[0]);\", (i+1)/SCROLL_PAUSES)\n",
    "            time.sleep(SCROLL_INTERVAL)\n",
    "    except WebDriverException:\n",
    "        # fallback simple\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_INTERVAL)\n",
    "\n",
    "def try_click_show_more(driver):\n",
    "    \"\"\"Intentar clicar botones 'mostrar/ver más' si existen (varios textos/selectores).\"\"\"\n",
    "    candidates_xpaths = [\n",
    "        \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'mostrar')]\",\n",
    "        \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'ver más')]\",\n",
    "        \"//a[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'ver más')]\",\n",
    "        \"//button[contains(., 'Load more') or contains(., 'See more') or contains(., 'Show more')]\",\n",
    "        \"//a[contains(., 'Load more') or contains(., 'See more') or contains(., 'Show more')]\"\n",
    "    ]\n",
    "    for xp in candidates_xpaths:\n",
    "        try:\n",
    "            elems = driver.find_elements(By.XPATH, xp)\n",
    "            for e in elems:\n",
    "                try:\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", e)\n",
    "                    time.sleep(0.15)\n",
    "                    driver.execute_script(\"arguments[0].click();\", e)\n",
    "                    time.sleep(0.4)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "def collect_links_on_page(driver):\n",
    "    \"\"\"Extrae enlaces desde DOM (JS) y via find_elements; normaliza.\"\"\"\n",
    "    hrefs = set()\n",
    "    # 1) JS collection\n",
    "    try:\n",
    "        js = \"\"\"\n",
    "        return Array.from(document.querySelectorAll('a[href]')).map(a=>a.href);\n",
    "        \"\"\"\n",
    "        all_hrefs = driver.execute_script(js)\n",
    "        if all_hrefs:\n",
    "            for h in all_hrefs:\n",
    "                if not isinstance(h, str):\n",
    "                    continue\n",
    "                # normalizar\n",
    "                h2 = re.split(r'[#?]', h)[0]\n",
    "                for pat in LINK_PATTERNS:\n",
    "                    if pat in h2:\n",
    "                        hrefs.add(h2)\n",
    "                        break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Selenium find_elements fallback\n",
    "    try:\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href]')\n",
    "        for e in elems:\n",
    "            try:\n",
    "                h = e.get_attribute(\"href\")\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not h:\n",
    "                continue\n",
    "            h2 = re.split(r'[#?]', h)[0]\n",
    "            for pat in LINK_PATTERNS:\n",
    "                if pat in h2:\n",
    "                    hrefs.add(h2)\n",
    "                    break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return list(hrefs)\n",
    "\n",
    "def extract_article_from_html(html, url):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = h1.get_text(strip=True) if h1 else \"\"\n",
    "    desc = \"\"\n",
    "    meta = soup.find(\"meta\", {\"name\":\"description\"})\n",
    "    if meta and meta.get(\"content\"):\n",
    "        desc = meta[\"content\"].strip()\n",
    "    else:\n",
    "        og = soup.find(\"meta\", {\"property\":\"og:description\"})\n",
    "        if og and og.get(\"content\"):\n",
    "            desc = og[\"content\"].strip()\n",
    "\n",
    "    body_parts = []\n",
    "    article_tag = soup.find(\"article\")\n",
    "    if article_tag:\n",
    "        blocks = article_tag.find_all(lambda tag: tag.name==\"div\" and tag.get(\"data-component\")==\"text-block\")\n",
    "        if blocks:\n",
    "            for b in blocks:\n",
    "                for p in b.find_all(\"p\"):\n",
    "                    t = p.get_text(\" \", strip=True)\n",
    "                    if t:\n",
    "                        body_parts.append(t)\n",
    "        else:\n",
    "            for p in article_tag.find_all(\"p\"):\n",
    "                t = p.get_text(\" \", strip=True)\n",
    "                if t:\n",
    "                    body_parts.append(t)\n",
    "    else:\n",
    "        main = soup.find(\"main\") or soup.find(\"div\", {\"role\":\"main\"})\n",
    "        if main:\n",
    "            for p in main.find_all(\"p\"):\n",
    "                t = p.get_text(\" \", strip=True)\n",
    "                if t:\n",
    "                    body_parts.append(t)\n",
    "        else:\n",
    "            for p in soup.find_all(\"p\"):\n",
    "                t = p.get_text(\" \", strip=True)\n",
    "                if t and len(t) > 30:\n",
    "                    body_parts.append(t)\n",
    "\n",
    "    body = \"\\n\\n\".join(body_parts).strip()\n",
    "    if not body:\n",
    "        return None\n",
    "\n",
    "    # limpiar boilerplate\n",
    "    body = clean_and_truncate_body(body)\n",
    "    body = remove_unwanted_lines(body)\n",
    "    body = re.sub(r'\\n{3,}', '\\n\\n', body).strip()\n",
    "\n",
    "    return {\"url\": url, \"title\": title, \"description\": desc, \"body\": body}\n",
    "\n",
    "def extract_article(driver, url):\n",
    "    \"\"\"Carga la URL en driver y extrae con BeautifulSoup la info del artículo.\"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] fallo cargando {url}: {e}\")\n",
    "        return None\n",
    "    time.sleep(0.5)\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, 350);\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(0.5)\n",
    "    html = driver.page_source\n",
    "    return extract_article_from_html(html, url)\n",
    "\n",
    "def clean_and_truncate_body(body_text: str) -> str:\n",
    "    lower = body_text.lower()\n",
    "    truncate_at = None\n",
    "    for phrase in TRUNCATE_PHRASES:\n",
    "        idx = lower.find(phrase.lower())\n",
    "        if idx != -1:\n",
    "            if truncate_at is None or idx < truncate_at:\n",
    "                truncate_at = idx\n",
    "    return body_text[:truncate_at].strip() if truncate_at is not None else body_text\n",
    "\n",
    "def remove_unwanted_lines(text: str) -> str:\n",
    "    lines = text.splitlines()\n",
    "    new_lines = []\n",
    "    for L in lines:\n",
    "        Ls = L.strip()\n",
    "        if not Ls:\n",
    "            continue\n",
    "        skip = False\n",
    "        for pat in REMOVE_LINE_PATTERNS:\n",
    "            if re.match(pat, Ls, flags=re.IGNORECASE):\n",
    "                skip = True\n",
    "                break\n",
    "        if skip:\n",
    "            continue\n",
    "        new_lines.append(Ls)\n",
    "    return \"\\n\".join(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d28a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_label(driver, base_topic_url, label_value, target_count):\n",
    "    collected = []\n",
    "    seen = set()\n",
    "\n",
    "    # 1) probar ?page=N (muchas pages posibles)\n",
    "    for p in range(1, MAX_PAGES+1):\n",
    "        if len(collected) >= target_count:\n",
    "            break\n",
    "        page_url = f\"{base_topic_url}?page={p}\"\n",
    "        print(f\"[INFO] cargando {page_url}\")\n",
    "        try:\n",
    "            driver.get(page_url)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] fallo página {page_url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # scroll agresivo y click en botones 'mostrar más'\n",
    "        aggressive_scroll(driver)\n",
    "        try_click_show_more(driver)\n",
    "\n",
    "        # esperar anchors renderizados (mínimo 1 anchor)\n",
    "        try:\n",
    "            WebDriverWait(driver, WAIT_TIMEOUT).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href]\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(\"  [WARN] timeout esperando anchors en la página\")\n",
    "\n",
    "        links = collect_links_on_page(driver)\n",
    "        print(f\"  [FOUND] {len(links)} links candidatos\")\n",
    "        for link in links:\n",
    "            if len(collected) >= target_count:\n",
    "                break\n",
    "            if link in seen:\n",
    "                continue\n",
    "            seen.add(link)\n",
    "            art = extract_article(driver, link)\n",
    "            if not art:\n",
    "                print(f\"    [SKIP] sin cuerpo extraíble: {link}\")\n",
    "                continue\n",
    "            id_ = short_id(link)\n",
    "            collected.append({\n",
    "                \"id\": id_, \"label\": label_value, \"title\": art[\"title\"],\n",
    "                \"description\": art[\"description\"], \"body\": art[\"body\"], \"url\": link\n",
    "            })\n",
    "            print(f\"    [ADD] {len(collected)}/{target_count}: {art['title'][:90]}\")\n",
    "            time.sleep(RATE_SLEEP)\n",
    "\n",
    "    # 2) si no alcanzó, intentar desde la landing sin ?page y clickear paginacion 'siguiente'\n",
    "    if len(collected) < target_count:\n",
    "        print(\"[INFO] intentando paginación por UI (botón 'Siguiente') como fallback\")\n",
    "        try:\n",
    "            driver.get(base_topic_url)\n",
    "            aggressive_scroll(driver)\n",
    "            try_click_show_more(driver)\n",
    "            # intentar iterar clicks en 'siguiente'\n",
    "            for attempt in range(1, MAX_PAGES+1):\n",
    "                # localizar un enlace visible que parezca 'Siguiente' o con 'page=' en href\n",
    "                cand = driver.find_elements(By.XPATH, \"//a[contains(@href,'?page=') or contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'), 'siguiente') or contains(., 'Next') or contains(., 'next')]\")\n",
    "                if not cand:\n",
    "                    break\n",
    "                el = cand[-1]\n",
    "                try:\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block:'center'});\", el)\n",
    "                    time.sleep(0.2)\n",
    "                    driver.execute_script(\"arguments[0].click();\", el)\n",
    "                except Exception:\n",
    "                    break\n",
    "                time.sleep(0.8)\n",
    "                aggressive_scroll(driver)\n",
    "                try_click_show_more(driver)\n",
    "                links = collect_links_on_page(driver)\n",
    "                print(f\"  [FOUND after click] {len(links)} links\")\n",
    "                for link in links:\n",
    "                    if len(collected) >= target_count:\n",
    "                        break\n",
    "                    if link in seen:\n",
    "                        continue\n",
    "                    seen.add(link)\n",
    "                    art = extract_article(driver, link)\n",
    "                    if not art:\n",
    "                        print(f\"    [SKIP] sin cuerpo extraíble: {link}\")\n",
    "                        continue\n",
    "                    id_ = short_id(link)\n",
    "                    collected.append({\n",
    "                        \"id\": id_, \"label\": label_value, \"title\": art[\"title\"],\n",
    "                        \"description\": art[\"description\"], \"body\": art[\"body\"], \"url\": link\n",
    "                    })\n",
    "                    print(f\"    [ADD] {len(collected)}/{target_count}: {art['title'][:90]}\")\n",
    "                    time.sleep(RATE_SLEEP)\n",
    "                if len(collected) >= target_count:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] paginación UI fallback falló:\", e)\n",
    "\n",
    "    return collected[:target_count]\n",
    "\n",
    "def save_as_json_txt(path: Path, objects):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(objects, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a802e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Recolectando FALSAS...\n",
      "[INFO] cargando https://www.bbc.com/mundo/topics/c95y3rnvxkwt?page=1\n",
      "  [FOUND] 24 links candidatos\n",
      "    [ADD] 1/25: Cómo el emoji de la zanahoria se convirtió en un código secreto en internet para camuflar \n",
      "    [ADD] 2/25: Cómo un viaje a Noruega me hizo ver que estaba equivocado y que la Tierra no es plana\n",
      "    [ADD] 3/25: Quiénes son los \"Ciudadanos del Reich\", el grupo asociado con los golpistas arrestados en \n",
      "    [ADD] 4/25: \"Mi foto fue usada para diseminar mentiras sobre la guerra\": la verdadera historia de una \n",
      "    [ADD] 5/25: La matanza que los soviéticos atribuyeron con éxito a los nazis durante 50 años\n",
      "    [ADD] 6/25: Las afirmaciones falsas y engañosas amplificadas por Elon Musk en Twitter\n",
      "    [ADD] 7/25: Rusia invade Ucrania: cómo saber si lo que estás viendo sobre el conflicto es real o son \"\n",
      "    [ADD] 8/25: Donald Trump: cómo detectar imágenes creadas por inteligencia artificial como las fotos fa\n",
      "    [ADD] 9/25: La princesa Kate pide disculpas por la \"confusión\" que causó su foto retirada por las agen\n",
      "    [ADD] 10/25: \"Alguien simuló mi muerte en TikTok\"\n",
      "    [ADD] 11/25: Los diabólicos orígenes de las teorías de conspiración (y el rey francés que creó una de l\n",
      "    [ADD] 12/25: Cómo una historia en México me llevó a destapar el mayor escándalo del periodismo alemán\n",
      "    [ADD] 13/25: Premio Pulitzer: los antecedentes amarillistas del creador de uno de los galardones más pr\n",
      "    [ADD] 14/25: Rusia y Ucrania: la sobreviviente de un bombardeo a una escuela ucraniana a la que Moscú \"\n",
      "    [ADD] 15/25: La madre de Katy Perry, engañada con una foto falsa de su hija en la Met Gala creada por i\n",
      "    [ADD] 16/25: Cómo Finlandia ha conseguido combatir con éxito las noticias falsas\n",
      "    [ADD] 17/25: “Me arrepiento de haber publicado en línea que yo era Madeleine McCann”\n",
      "    [ADD] 18/25: Los sobrevivientes de ataques terroristas que son acusados de fingir por teóricos de la co\n",
      "    [ADD] 19/25: Rusia y Ucrania | Investigación BBC: los misteriosos grupos de Facebook que apoyan a Putin\n",
      "    [ADD] 20/25: Lula vs Bolsonaro | \"Los pastores evangélicos están sembrando el miedo desde el altar\"\n",
      "    [ADD] 21/25: El Yimeilun, la estrategia con la que China busca influir en las elecciones de Taiwán al s\n",
      "    [ADD] 22/25: Las imágenes falsas creadas con IA para intentar atraer el apoyo de los votantes negros ha\n",
      "    [ADD] 23/25: \"Nuestra hermana murió por las teorías conspirativas de nuestra madre sobre el cáncer\"\n",
      "    [ADD] 24/25: Guerra en Ucrania: los \"rostros robados\" que se usan para promocionar y defender a Vladimi\n",
      "[INFO] cargando https://www.bbc.com/mundo/topics/c95y3rnvxkwt?page=2\n",
      "  [FOUND] 22 links candidatos\n",
      "    [ADD] 25/25: Vacunas contra el coronavirus: \"Cómo una foto de mi pie se convirtió en propaganda antivac\n",
      "[RUN] Recolectando VERDADERAS...\n",
      "[INFO] cargando https://www.bbc.com/mundo/topics/c7zp57yyz25t?page=1\n",
      "  [FOUND] 24 links candidatos\n",
      "    [ADD] 1/25: En el mundo hay más niños con sobrepeso que con bajo peso por primera vez en la historia: \n",
      "    [ADD] 2/25: La \"superautopista\" marina que los carteles sudamericanos usan para enviar droga a Reino U\n",
      "    [ADD] 3/25: Del zapato de Kruschev al \"huele a azufre\" de Chávez: 8 de los discursos más polémicos en \n",
      "    [ADD] 4/25: \"El olor a cadáver era espantoso. Eso no se te olvida nunca\": 40 años del devastador terre\n",
      "    [ADD] 5/25: \"Se veían muchos muertos en la selva\": los cientos de cuerpos recuperados en el Darién que\n",
      "    [ADD] 6/25: \"La presencia militar de EE.UU. en el Caribe es una desproporción. La mayoría del narcotrá\n",
      "    [ADD] 7/25: Por qué Costa Rica se convirtió en la nación con el mayor aumento de millonarios extranjer\n",
      "    [ADD] 8/25: 30 años del \"escorpión\" del arquero colombiano René Higuita en Wembley, la jugada que hech\n",
      "    [ADD] 9/25: \"Si ponen en peligro nuestra seguridad, serán derribados\": la advertencia de Trump a Madur\n",
      "    [ADD] 10/25: Por qué la sentencia a la cúpula de las FARC y a 12 exmilitares es un hito en Colombia (y \n",
      "    [ADD] 11/25: Por qué los ataques militares de EE.UU. a barcos de Venezuela son vistos como una escalada\n",
      "    [ADD] 12/25: Jair Bolsonaro condenado: el plan golpista que hundió al expresidente de Brasil y cómo que\n",
      "    [ADD] 13/25: Cómo Juancho, un caimán del Orinoco, juega desde Dallas un papel clave para impedir la ext\n",
      "    [ADD] 14/25: Grito de Dolores: 5 mitos y verdades del hito que marca el inicio de la independencia de M\n",
      "    [ADD] 15/25: Por qué el despliegue del ejército de EE.UU. en el Caribe y los ejercicios militares reavi\n",
      "    [ADD] 16/25: Trump afirma que EE.UU. ya destruyó 3 embarcaciones con drogas procedentes de Venezuela\n",
      "    [ADD] 17/25: \"No corro, no grito, no empujo\": qué cambió en Ciudad de México tras el devastador terremo\n",
      "    [ADD] 18/25: Bolivia aprueba una histórica ley que prohíbe sin excepciones el matrimonio con menores de\n",
      "    [ADD] 19/25: \"Trump es el presidente de Estados Unidos, no el emperador del mundo\": Luiz Inácio Lula da\n",
      "    [ADD] 20/25: El día que las mujeres se rebelaron contra el servicio militar obligatorio en Brasil\n",
      "    [ADD] 21/25: Qué significa que EE.UU. retire a Colombia la certificación en la lucha contra el narcotrá\n",
      "    [ADD] 22/25: Qué se sabe del Cartel de los Soles, la organización designada como terrorista por EE.UU. \n",
      "    [ADD] 23/25: El Supremo de Brasil condena a Bolsonaro  a 27 años de prisión por intento de golpe de Est\n",
      "    [ADD] 24/25: \"Una cosa es que la muerte en México se represente en toda una serie de aspectos y otra es\n",
      "[INFO] cargando https://www.bbc.com/mundo/topics/c7zp57yyz25t?page=2\n",
      "  [FOUND] 24 links candidatos\n",
      "    [ADD] 25/25: Henrique Capriles: \"La mayor parte de las personas que quieren una invasión de Estados Uni\n",
      "[RESULT] Falsas reunidas: 25; Verdaderas reunidas: 25\n",
      "[OK] Guardados: corpus_bbc_mundo\\Falsas_25.txt  y  corpus_bbc_mundo\\Verdaderas_25.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # set headless=False if quieres ver el navegador y debuggear\n",
    "    driver = create_driver(headless=False)\n",
    "    try:\n",
    "        print(\"[RUN] Recolectando FALSAS...\")\n",
    "        falsos = gather_label(driver, TOPIC_FALSE, LABEL_FALSE, TARGET_PER_LABEL)\n",
    "        print(\"[RUN] Recolectando VERDADERAS...\")\n",
    "        verdaderos = gather_label(driver, TOPIC_TRUE, LABEL_TRUE, TARGET_PER_LABEL)\n",
    "\n",
    "        print(f\"[RESULT] Falsas reunidas: {len(falsos)}; Verdaderas reunidas: {len(verdaderos)}\")\n",
    "        if len(falsos) < TARGET_PER_LABEL or len(verdaderos) < TARGET_PER_LABEL:\n",
    "            print(\"[WARN] No se alcanzó la cantidad objetivo. Ajusta MAX_PAGES/SCROLL_PAUSES o ejecuta con headless=False para investigar.\")\n",
    "        save_as_json_txt(OUT_FALSE_FILE, falsos)\n",
    "        save_as_json_txt(OUT_TRUE_FILE, verdaderos)\n",
    "        print(f\"[OK] Guardados: {OUT_FALSE_FILE}  y  {OUT_TRUE_FILE}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# test para subir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
