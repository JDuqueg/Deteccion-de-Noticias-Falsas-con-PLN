{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19075de6",
   "metadata": {},
   "source": [
    "# Web Scraping (El País + Fuentes Satíricas)\n",
    "\n",
    "Objetivo: Obtener un conjunto balanceado de 25 noticias reales y 25 satíricas para experimentos de detección de noticias falsas.\n",
    "\n",
    "Formato final (simplificado):\n",
    "- Dos únicos archivos JSON (array) en `corpus/Jonatan/{Verdad,Falso}`:\n",
    "  - `Verdad/Verdad.txt`\n",
    "  - `Falso/Falso.txt`\n",
    "- Cada elemento del array contiene: `id`, `label` ('1'=Verdad, '0'=Falso), `title`, `description`, `body`, `url`, `topic`.\n",
    "- Metadatos tabulares (auditoría) en `data/interim/metadata_scraping_Jonatan.csv`.\n",
    "\n",
    "Secciones del notebook:\n",
    "1. Configuración (rutas y parámetros)\n",
    "2. Seeds (fuentes de descubrimiento)\n",
    "3. Motor de scraping (descubrimiento + extracción + acumulación en memoria)\n",
    "4. Ejecución (genera los dos archivos finales) + Migración rutas antiguas\n",
    "5. Validación (verifica estructura de Verdad.txt y Falso.txt)\n",
    "\n",
    "Principios:\n",
    "- Reproducibilidad con rutas relativas.\n",
    "- Control de duplicados por URL y hash parcial.\n",
    "- Heurística simple para inferir `topic`.\n",
    "- Etiquetas serializadas como cadenas '1'/'0'.\n",
    "\n",
    "Instruciones rápidas:\n",
    "1. Ajustar umbrales si es necesario.\n",
    "2. Ejecutar en orden hasta Ejecución.\n",
    "3. Correr Validación.\n",
    "4. Consumir los dos archivos finales o fusionarlos externamente según necesidad.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd20e72",
   "metadata": {},
   "source": [
    "## Nota de esquema final\n",
    "\n",
    "Archivos de salida:\n",
    "- `corpus/Jonatan/Verdad/Verdad.txt`\n",
    "- `corpus/Jonatan/Falso/Falso.txt`\n",
    "\n",
    "Cada archivo es un array JSON de objetos con las claves:\n",
    "```\n",
    "{\n",
    "  \"id\": str,\n",
    "  \"label\": \"1\" | \"0\",\n",
    "  \"title\": str,\n",
    "  \"description\": str,\n",
    "  \"body\": str,\n",
    "  \"url\": str,\n",
    "  \"topic\": str\n",
    "}\n",
    "```\n",
    "`label` se serializa como cadena ('1' verdad, '0' falso). `topic` se infiere heurísticamente por palabras clave. La descripción (`description`) toma las primeras oraciones / fragmentos del cuerpo.\n",
    "\n",
    "Metadatos: `data/interim/metadata_scraping_Jonatan.csv` (append incremental) con columnas: timestamp,label,domain,url,title,n_words,topic.\n",
    "\n",
    "Validación: La celda de validación asegura estructura correcta y normaliza si fuese necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210b630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raíz: c:\\GitHub\\unal-pln-lab\\practica1-noticias-falsas\n",
      "Salida Verdad: data\\raw\\jontan\\Verdad\n",
      "Salida Falso : data\\raw\\jontan\\Falso\n",
      "Metadatos    : data\\interim\\metadata_scraping_Jonatan.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuración y paths (revertido: se reintroduce 'description')\n",
    "from pathlib import Path\n",
    "import re, csv, json, hashlib, time\n",
    "from datetime import datetime\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "INTERIM_DIR = DATA_DIR / 'interim'\n",
    "CORPUS_DIR = PROJECT_ROOT / 'corpus'\n",
    "JONTAN_DIR = CORPUS_DIR / 'Jonatan'\n",
    "VERDAD_DIR = JONTAN_DIR / 'Verdad'\n",
    "FALSO_DIR = JONTAN_DIR / 'Falso'\n",
    "VERDAD_FILE = VERDAD_DIR / 'Verdad.txt'\n",
    "FALSO_FILE = FALSO_DIR / 'Falso.txt'\n",
    "METADATA_FILE = JONTAN_DIR / 'metadata.csv'\n",
    "\n",
    "for d in [RAW_DIR, INTERIM_DIR, VERDAD_DIR, FALSO_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_VERDAD = 25\n",
    "TARGET_FALSO = 25\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "# Documentación de columnas (description restaurado)\n",
    "COLUMN_DOC = {\n",
    "    'id': 'Identificador único interno (string)',\n",
    "    'label': \"Etiqueta '1' verdad, '0' falso (string)\",\n",
    "    'title': 'Título normalizado (string)',\n",
    "    'description': 'Resumen corto / lead (string)',\n",
    "    'body': 'Contenido principal del artículo (string)',\n",
    "    'url': 'URL original de la noticia (string)',\n",
    "    'topic': 'Tema inferido o asignado (string)'\n",
    "}\n",
    "\n",
    "HEADER_COMMENT_BLOCK = \"# Metadata columnas\\n\" + \"\\n\".join(f\"# {k}: {v}\" for k,v in COLUMN_DOC.items())\n",
    "\n",
    "# Regenerar metadata.csv con description\n",
    "with METADATA_FILE.open('w', encoding='utf-8', newline='') as f:\n",
    "    f.write(HEADER_COMMENT_BLOCK + '\\n')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(COLUMN_DOC.keys())\n",
    "    writer.writerow(['example_id','1','Ejemplo título','Ejemplo breve descripción','Cuerpo ejemplo','https://ejemplo','general'])\n",
    "\n",
    "print('Metadata restaurado con description:', METADATA_FILE)\n",
    "\n",
    "SLUG_RE = re.compile(r'[^a-z0-9]+')\n",
    "REQUIRED_TAGS = ['p']\n",
    "REQUIRED_KEYS = {'id','label','title','description','body','url','topic'}\n",
    "SEEN_URLS = set()\n",
    "SEEN_HASHES = set()\n",
    "\n",
    "MIN_PALABRAS_REAL = 120\n",
    "MIN_PALABRAS_FALSO_STRICT = 80\n",
    "MIN_PALABRAS_FALSO_RELAX = 40\n",
    "\n",
    "SKIP_SUFFIXES = {'.jpg','.jpeg','.png','.gif','.mp4','.webp'}\n",
    "MEDIA_EXT = tuple(SKIP_SUFFIXES)\n",
    "SKIP_EXACT = {'/favicon.ico','/robots.txt'}\n",
    "\n",
    "ARTICLE_PATTERNS = [r'/[0-9]{4}/[0-9]{2}/', r'/noticia', r'/news', r'/articulo']\n",
    "PATTERNS = [re.compile(p) for p in ARTICLE_PATTERNS]\n",
    "\n",
    "TOPIC_KEYWORDS = [\n",
    "    ('politica', ['elección','gobierno','senado','congreso','presidente','ministro','partido']),\n",
    "    ('economia', ['economía','inflación','mercado','bolsa','dólar','peso','finanzas']),\n",
    "    ('salud', ['salud','covid','virus','hospital','médico','enfermedad']),\n",
    "    ('deporte', ['fútbol','deporte','liga','partido','goles','torneo']),\n",
    "    ('tecnologia', ['tecnología','software','ia','inteligencia artificial','app','plataforma','ciberseguridad'])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5192a9b",
   "metadata": {},
   "source": [
    "## 2. Fuentes (seeds) y justificación\n",
    "\n",
    "En esta sección se listan las URLs de partida para descubrir artículos:\n",
    "- El País: secciones temáticas variadas para cubrir distintas áreas informativas.\n",
    "- Sitios satíricos: portada y secciones para maximizar diversidad de piezas.\n",
    "\n",
    "Los umbrales de longitud ayudan a filtrar contenidos demasiado breves que no aportarían suficiente contexto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e6dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2. SEEDS ====\n",
    "\"\"\"Listado de URLs semilla para descubrimiento de artículos.\n",
    "Reales: El País (varias secciones para diversidad temática).\n",
    "Satíricas: Actualidad Panamericana, El Panfleto (portada + secciones).\"\"\"\n",
    "\n",
    "ELPAIS_SEEDS = [\n",
    "    \"https://elpais.com/america/\",\n",
    "    \"https://elpais.com/ultimas-noticias/\",\n",
    "    \"https://elpais.com/internacional/\",\n",
    "    \"https://elpais.com/economia/\",\n",
    "    \"https://elpais.com/ciencia/\",\n",
    "    \"https://elpais.com/tecnologia/\"\n",
    "]\n",
    "\n",
    "FAKE_SEEDS = [\n",
    "    \"https://www.actualidadpanamericana.com/\",\n",
    "    \"https://elpanfleto.pe/\",\n",
    "    \"https://elpanfleto.pe/politica/\",\n",
    "    \"https://elpanfleto.pe/internacional/\",\n",
    "    \"https://elpanfleto.pe/sociedad/\"\n",
    "]\n",
    "print('Seeds reales:', len(ELPAIS_SEEDS), '| Seeds falsas:', len(FAKE_SEEDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e062a89",
   "metadata": {},
   "source": [
    "## 3. Motor de scraping (descubrimiento, filtrado y extracción)\n",
    "\n",
    "Componentes principales:\n",
    "- discover: recoge enlaces candidatos desde cada seed.\n",
    "- is_article: aplica heurísticas de forma de URL para filtrar.\n",
    "- extract_text_title: obtiene título y cuerpo limpio.\n",
    "- harvest: coordina el proceso en dos ciclos de longitud para alcanzar el objetivo.\n",
    "\n",
    "Se controla la duplicidad por URL y por hash parcial del contenido para evitar almacenar el mismo texto dos veces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e85af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3. MOTOR SIMPLE ====\n",
    "\"\"\"Motor minimalista de scraping con guardado TXT y JSON.\"\"\"\n",
    "ARTICLE_PATTERNS = [r\"/\\d{4}/\\d{2}/\\d{2}/\", r\"/noticia\", r\"/politica\", r\"/sociedad\", r\"/mundo\", r\"/america\", r\"/econom\", r\"/cultura\", r\"/tecnologia\", r\"/actualidad\", r\"/internacional\"]\n",
    "PATTERNS = [re.compile(p) for p in ARTICLE_PATTERNS]\n",
    "FAKE_DOMAINS = {\"www.actualidadpanamericana.com\", \"actualidadpanamericana.com\", \"elpanfleto.pe\", \"www.elpanfleto.pe\"}\n",
    "MEDIA_EXT = ('.jpg','.jpeg','.png','.gif','.mp4','.webp','.pdf','.zip')\n",
    "SLUG_RE = re.compile(r\"[a-z0-9-]{6,}\")\n",
    "SEEN_URLS, SEEN_HASHES = set(), set()\n",
    "\n",
    "# Heurística básica de tópicos por palabras clave. Orden importa.\n",
    "TOPIC_KEYWORDS = [\n",
    "    (\"salud\", [\"salud\",\"hospital\",\"clínic\",\"virus\",\"covid\",\"vacun\",\"enfermedad\",\"epidem\",\"médic\",\"sars\"]),\n",
    "    (\"politica\", [\"president\",\"congres\",\"senad\",\"ministro\",\"gobiern\",\"partido\",\"eleccion\",\"alcald\",\"candidato\",\"parlament\"]),\n",
    "    (\"economia\", [\"econom\",\"inflación\",\"dólar\",\"peso\",\"finanzas\",\"mercado\",\"banco\",\"inversión\",\"crédito\",\"tribut\"]),\n",
    "    (\"deportes\", [\"gol\",\"partido\",\"liga\",\"jugador\",\"técnico\",\"mundial\",\"torneo\",\"marcador\",\"selección\"]),\n",
    "    (\"tecnologia\", [\"tecnolog\",\"software\",\"ciber\",\"app\",\"plataforma\",\"innovación\",\"startup\",\"ia \",\"inteligencia artificial\",\"algoritmo\"]),\n",
    "    (\"cultura\", [\"museo\",\"arte\",\"cultural\",\"festival\",\"cine\",\"teatro\",\"exposición\",\"literatura\",\"música\"]),\n",
    "    (\"ambiente\", [\"clima\",\"ambiental\",\"bosque\",\"deforest\",\"emisiones\",\"carbono\",\"calentamiento\",\"sostenible\",\"biodiversidad\"]),\n",
    "]\n",
    "\n",
    "def infer_topic(title: str, text: str) -> str:\n",
    "    blob = f\"{title}\\n{text[:2000]}\".lower()\n",
    "    for topic, kws in TOPIC_KEYWORDS:\n",
    "        for kw in kws:\n",
    "            if kw in blob:\n",
    "                return topic\n",
    "    return \"general\"\n",
    "\n",
    "def get_html(url: str) -> str:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "    r.raise_for_status(); return r.text\n",
    "\n",
    "def is_article(url: str) -> bool:\n",
    "    p = urlparse(url); path = p.path.lower()\n",
    "    if not path or path == '/' or path.endswith(MEDIA_EXT): return False\n",
    "    if any(rx.search(path) for rx in PATTERNS): return True\n",
    "    if p.netloc in FAKE_DOMAINS:\n",
    "        parts = [x for x in path.strip('/').split('/') if x]\n",
    "        if 1 <= len(parts) <= 3:\n",
    "            last = parts[-1]\n",
    "            if '-' in last and SLUG_RE.fullmatch(last) and len(last) >= 8:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def discover(seed: str, limit: int) -> list[str]:\n",
    "    try: html = get_html(seed)\n",
    "    except Exception: return []\n",
    "    soup = BeautifulSoup(html, 'lxml'); out, seen = [], set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        full = urljoin(seed, a['href'])\n",
    "        if full in seen: continue\n",
    "        seen.add(full)\n",
    "        if is_article(full):\n",
    "            out.append(full)\n",
    "            if len(out) >= limit: break\n",
    "    return out\n",
    "\n",
    "def extract_text_title(html: str) -> tuple[str,str]:\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    for t in soup(['script','style','noscript']): t.decompose()\n",
    "    art = soup.find('article')\n",
    "    ps = art.find_all('p') if art else soup.find_all('p')\n",
    "    chunks = [p.get_text(' ', strip=True) for p in ps if len(p.get_text().split()) >= 4]\n",
    "    text = re.sub(r'\\s+',' ',' '.join(chunks)).strip()\n",
    "    title_tag = soup.find('h1'); title = title_tag.get_text(' ', strip=True) if title_tag else ''\n",
    "    return text, title\n",
    "\n",
    "def build_name(title: str, label: str, domain: str) -> str:\n",
    "    slug = slugify(title)[:70] if title else 'sin_titulo'\n",
    "    stamp = datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%S')\n",
    "    return f\"{stamp}__{domain.replace('.','_')}__{slug}__{'verdad' if label=='Verdad' else 'falso'}\"\n",
    "\n",
    "def save_article(text: str, meta: dict) -> tuple[str,str]:\n",
    "    out_dir = VERDAD_DIR if meta['label']=='Verdad' else FALSO_DIR\n",
    "    base = build_name(meta['title'], meta['label'], meta['domain'])\n",
    "    txt_path = out_dir / f\"{base}.txt\"\n",
    "    json_path = out_dir / f\"{base}.json\"\n",
    "    topic = infer_topic(meta['title'], text)\n",
    "    meta['topic'] = topic\n",
    "    # Formato solicitado\n",
    "    # [SOURCE:...]\\n[URL:...]\\n[LABEL:...]\\n[TOPIC:...]\\n\\nTEXTO\n",
    "    with open(txt_path,'w',encoding='utf-8') as f:\n",
    "        f.write(f\"[SOURCE:{meta['domain']}]\\n\")\n",
    "        f.write(f\"[URL:{meta['url']}]\\n\")\n",
    "        f.write(f\"[LABEL:{meta['label']}]\\n\")\n",
    "        f.write(f\"[TOPIC:{topic}]\\n\")\n",
    "        f.write(f\"[TIMESTAMP_UTC:{ts()}]\\n\")\n",
    "        f.write(f\"[N_WORDS:{meta['n_words']}]\\n\")\n",
    "        if meta['title']:\n",
    "            f.write(f\"[TITLE:{meta['title']}]\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(text)\n",
    "    json_obj = {**meta,'text': text}\n",
    "    with open(json_path,'w',encoding='utf-8') as jf:\n",
    "        json.dump(json_obj, jf, ensure_ascii=False, indent=2)\n",
    "    return txt_path.name, json_path.name\n",
    "\n",
    "COLUMN_DOC = {\n",
    "    'timestamp': 'Fecha/hora UTC ISO',\n",
    "    'label': 'Clase (Verdad/Falso)',\n",
    "    'domain': 'Dominio origen',\n",
    "    'url': 'URL del artículo',\n",
    "    'title': 'Título extraído',\n",
    "    'n_words': 'Conteo de palabras',\n",
    "    'filename_txt': 'Archivo texto plano',\n",
    "    'filename_json': 'Archivo JSON con texto+meta',\n",
    "    'topic': 'Tópico heurístico inferido'\n",
    "}\n",
    "HEADER_COMMENT_BLOCK = \"# METADATA FILE (Descripción de columnas)\\n\" + \"\\n\".join([f\"# {k}: {v}\" for k,v in COLUMN_DOC.items()]) + \"\\n\"\n",
    "\n",
    "def _ensure_metadata_upgraded():\n",
    "    if not METADATA_FILE.exists():\n",
    "        return\n",
    "    # Detectar si cabecera antigua no tiene 'topic'\n",
    "    with open(METADATA_FILE,'r',encoding='utf-8') as f:\n",
    "        header_line = None\n",
    "        for ln in f:\n",
    "            if ln.startswith('#') or not ln.strip():\n",
    "                continue\n",
    "            header_line = ln.strip()\n",
    "            break\n",
    "    if header_line and 'topic' not in header_line.split(','):\n",
    "        try:\n",
    "            df_old = pd.read_csv(METADATA_FILE, comment='#')\n",
    "            if 'topic' not in df_old.columns:\n",
    "                df_old['topic'] = 'unknown'\n",
    "            with open(METADATA_FILE,'w',encoding='utf-8') as f:\n",
    "                f.write(HEADER_COMMENT_BLOCK)\n",
    "            df_old.to_csv(METADATA_FILE, mode='a', index=False)\n",
    "            print('Metadata existente actualizada para incluir columna topic (rellenada con \"unknown\").')\n",
    "        except Exception as e:\n",
    "            print('No se pudo actualizar metadata previa:', e)\n",
    "\n",
    "\n",
    "def append_metadata(rows: list[dict]):\n",
    "    if not rows: return\n",
    "    _ensure_metadata_upgraded()\n",
    "    cols = ['timestamp','label','domain','url','title','n_words','filename_txt','filename_json','topic']\n",
    "    df_new = pd.DataFrame(rows, columns=cols)\n",
    "    if METADATA_FILE.exists():\n",
    "        df_new.to_csv(METADATA_FILE, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        with open(METADATA_FILE,'w',encoding='utf-8') as f: f.write(HEADER_COMMENT_BLOCK)\n",
    "        df_new.to_csv(METADATA_FILE, mode='a', index=False)\n",
    "    try:\n",
    "        with open(METADATA_FILE,'r',encoding='utf-8') as f:\n",
    "            lines = [ln for ln in f.readlines() if not ln.startswith('#') and ln.strip()]\n",
    "        total = len(lines)-1 if lines else 0\n",
    "        print('Metadatos total (registros):', total)\n",
    "    except Exception:\n",
    "        print('Metadatos actualizados.')\n",
    "\n",
    "def harvest(seeds: list[str], label: str, objetivo: int, min_words_strict: int, min_words_relax: int | None = None, discover1: int = 120, discover2: int = 220) -> int:\n",
    "    guardadas = 0; filas: list[dict] = []\n",
    "    fases = [(min_words_strict, discover1)] if not min_words_relax else [(min_words_strict, discover1),(min_words_relax, discover2)]\n",
    "    for fase_idx,(min_w,disc_lim) in enumerate(fases, start=1):\n",
    "        if guardadas >= objetivo: break\n",
    "        print(f\"Fase {fase_idx} {label} (min_w={min_w}, discover={disc_lim})\")\n",
    "        for seed in seeds:\n",
    "            if guardadas >= objetivo: break\n",
    "            links = discover(seed, disc_lim); random.shuffle(links)\n",
    "            for url in links:\n",
    "                if guardadas >= objetivo: break\n",
    "                if url in SEEN_URLS: continue\n",
    "                try:\n",
    "                    html = get_html(url); text, title = extract_text_title(html)\n",
    "                    if len(text.split()) < min_w: continue\n",
    "                    h = hash(text[:8000]);\n",
    "                    if h in SEEN_HASHES: continue\n",
    "                    SEEN_HASHES.add(h); SEEN_URLS.add(url)\n",
    "                    domain = urlparse(url).netloc\n",
    "                    meta = {'timestamp': ts(),'label': label,'domain': domain,'url': url,'title': title,'n_words': len(text.split())}\n",
    "                    fname_txt, fname_json = save_article(text, meta)\n",
    "                    meta['filename_txt'] = fname_txt; meta['filename_json'] = fname_json\n",
    "                    filas.append(meta); guardadas += 1\n",
    "                    if guardadas % 5 == 0: print(f\"{label}: {guardadas}\")\n",
    "                    time.sleep(0.6)\n",
    "                except Exception:\n",
    "                    time.sleep(0.4)\n",
    "    append_metadata(filas); return guardadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa7b54",
   "metadata": {},
   "source": [
    "## 4. Ejecución del proceso y reporte\n",
    "\n",
    "En este paso se ejecutan dos llamadas a harvest:\n",
    "1. Recolección de artículos reales de El País.\n",
    "2. Recolección de artículos satíricos con un segundo ciclo de menor umbral de longitud si aún faltan.\n",
    "\n",
    "Al finalizar se imprime un resumen y las rutas relativas de los resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando scraping...\n",
      "Fase 1 Verdad (min_w=130, discover=120)\n",
      "Verdad: 5\n",
      "Verdad: 5\n"
     ]
    }
   ],
   "source": [
    "# ==== 4. EJECUCIÓN ====\n",
    "print('Iniciando scraping...')\n",
    "real = harvest(ELPAIS_SEEDS, 'Verdad', TARGET_VERDAD, MIN_PALABRAS_REAL)\n",
    "fake = harvest(FAKE_SEEDS, 'Falso', TARGET_FALSO, MIN_PALABRAS_FALSO_STRICT, MIN_PALABRAS_FALSO_RELAX)\n",
    "print('=== Resumen ===')\n",
    "print(f'Verdad: {real}/{TARGET_VERDAD}')\n",
    "print(f'Falso : {fake}/{TARGET_FALSO}')\n",
    "print('Rutas relativas:')\n",
    "print(' - Verdad TXT/JSON:', VERDAD_DIR.relative_to(PROJECT_ROOT))\n",
    "print(' - Falso  TXT/JSON:', FALSO_DIR.relative_to(PROJECT_ROOT))\n",
    "print('Metadatos:', METADATA_FILE.relative_to(PROJECT_ROOT))\n",
    "if real < TARGET_VERDAD: print('[Aviso] Ajustar MIN_PALABRAS_REAL o añadir seeds El País.')\n",
    "if fake < TARGET_FALSO: print('[Aviso] Ajustar umbrales o añadir fuentes satíricas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidad: combinar artículos por clase en un único TXT\n",
    "from pathlib import Path\n",
    "\n",
    "def combine_class_files(src_dir: Path, pattern: str, out_filename: str):\n",
    "    out_path = src_dir / out_filename\n",
    "    txt_files = sorted([p for p in src_dir.glob(pattern) if p.is_file() and p.suffix == '.txt'])\n",
    "    if not txt_files:\n",
    "        print(f\"No se encontraron archivos para combinar en {src_dir}\")\n",
    "        return None\n",
    "    with open(out_path, 'w', encoding='utf-8') as out:\n",
    "        for i, f in enumerate(txt_files, start=1):\n",
    "            try:\n",
    "                content = f.read_text(encoding='utf-8')\n",
    "            except Exception:\n",
    "                continue\n",
    "            out.write(f\"===== INICIO ARTÍCULO {i} : {f.name} =====\\n\")\n",
    "            out.write(content.strip() + \"\\n\")\n",
    "            out.write(f\"===== FIN ARTÍCULO {i} =====\\n\\n\")\n",
    "    print(f\"Combinado -> {out_path.relative_to(PROJECT_ROOT)} ({len(txt_files)} artículos)\")\n",
    "    return out_path\n",
    "\n",
    "# Ejecutar combinación (se rehace cada vez)\n",
    "combine_class_files(VERDAD_DIR, '*.txt', 'VERDAD_combinado.txt')\n",
    "combine_class_files(FALSO_DIR, '*.txt', 'FALSO_combinado.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac40ef4",
   "metadata": {},
   "source": [
    "## 5. Agregación en archivos únicos por clase\n",
    "\n",
    "Esta sección combina todas las noticias guardadas en múltiples `.txt` individuales en un único archivo por clase:\n",
    "- `VERDAD_combinado.txt` dentro de la carpeta Verdad.\n",
    "- `FALSO_combinado.txt` dentro de la carpeta Falso.\n",
    "\n",
    "Formato de separación entre artículos:\n",
    "```\n",
    "===== INICIO ARTÍCULO n =====\n",
    "...contenido...\n",
    "===== FIN ARTÍCULO n =====\n",
    "```\n",
    "\n",
    "Mantiene intactos los archivos originales individuales y no duplica contenido si se vuelve a ejecutar (rehace el archivo desde cero).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd93148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== VALIDACIÓN ====\n",
    "import json\n",
    "REQUIRED_KEYS = {\"id\",\"label\",\"title\",\"description\",\"body\",\"url\",\"topic\"}\n",
    "files_to_check = [ (VERDAD_DIR/'Verdad.txt','Verdad'), (FALSO_DIR/'Falso.txt','Falso') ]\n",
    "issues = []\n",
    "for path,label in files_to_check:\n",
    "    if not path.exists():\n",
    "        issues.append((path.name,'missing'))\n",
    "        continue\n",
    "    try:\n",
    "        data = json.loads(path.read_text(encoding='utf-8'))\n",
    "        if not isinstance(data,list):\n",
    "            issues.append((path.name,'no_array'))\n",
    "            continue\n",
    "        changed = False\n",
    "        for i,obj in enumerate(data):\n",
    "            miss = REQUIRED_KEYS - set(obj.keys())\n",
    "            if miss:\n",
    "                issues.append((path.name,f'missing_keys_idx_{i}',sorted(miss)))\n",
    "            # normalizar label\n",
    "            if obj.get('label') not in {'0','1'}:\n",
    "                lb = obj.get('label')\n",
    "                if isinstance(lb,int):\n",
    "                    obj['label'] = '1' if lb==1 else '0'; changed=True\n",
    "                else:\n",
    "                    val = str(lb).lower()\n",
    "                    obj['label'] = '1' if 'verdad' in val else '0'; changed=True\n",
    "            if 'topic' not in obj:\n",
    "                obj['topic'] = infer_topic(obj.get('title',''), obj.get('body',''))\n",
    "                changed=True\n",
    "        if changed:\n",
    "            with open(path,'w',encoding='utf-8') as f:\n",
    "                json.dump(data,f,ensure_ascii=False,indent=2)\n",
    "    except json.JSONDecodeError as e:\n",
    "        issues.append((path.name,'json_error',str(e)))\n",
    "    except Exception as e:\n",
    "        issues.append((path.name,'error',str(e)))\n",
    "\n",
    "print('Total issues:', len(issues))\n",
    "if issues:\n",
    "    for it in issues[:12]:\n",
    "        print('  ', it)\n",
    "    if len(issues)>12: print('  ... (truncado)')\n",
    "else:\n",
    "    print('Validación OK (Verdad.txt y Falso.txt correctos).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
